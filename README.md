# Multi-agent RL low-fidelity test-bed

The Dynamic RL Test-bed is a low-fidelity environment aimed at evaluating the impact of various dynamic environments/sequential social dilemmas on multi-agent performance. This test-bed provides a platform for assessing the effectiveness of different strategies in dynamic settings, contributing to the advancement of reinforcement learning techniques in complex and changing environments.

Harvest and Clean-up environments can be found at original repo: https://github.com/eugenevinitsky/sequential_social_dilemma_games

PPO model (from scratch) developed using the following repo as reference: https://github.com/ericyangyu/PPO-for-Beginners/tree/master

## Key Features
- Low-fidelity simulation environment
- Multi-agent system support
- Dynamic environment embeddings for evaluating agent performance
- Easy-to-use interface for running experiments and analyzing results

## Usage
1. Clone the repository to your local machine.
2. Install the necessary dependencies using `pip install -r requirements.txt`.
3. Modify the configuration files to set up your experiment parameters.
4. Run the experiments using the provided scripts or directly from the command line.
5. Analyze the results and compare the performance of different dynamic environment embeddings.

## Contributions
Contributions to the Dynamic RL Test-bed project are welcome! Feel free to fork the repository, make improvements, and submit pull requests. Together, we can enhance the capabilities of this test-bed and further our understanding of dynamic environment interactions in multi-agent systems.

## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
